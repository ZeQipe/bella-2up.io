"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö (ChromaDB)
–†–µ–∞–ª–∏–∑—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
"""
import os
import hashlib
from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
import structlog
import chromadb
from chromadb.config import Settings

from ..config import config
from .embedding_service import embedding_service, EmbeddingServiceError

logger = structlog.get_logger()


class VectorServiceInterface(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
    
    @abstractmethod
    def initialize_knowledge_base_sync(self, kb_path: str) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        pass
    
    @abstractmethod
    async def search_similar(self, query: str, limit: int = None, threshold: float = None) -> List[Dict]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        pass
    
    @abstractmethod
    async def add_document(self, content: str, metadata: Dict) -> str:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
        pass


class ChromaVectorService(VectorServiceInterface):
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ChromaDB"""
    
    def __init__(self):
        self.db_path = config.CHROMA_DB_PATH
        self.collection_name = "casino_knowledge_base"
        self.embedding_model = config.OPENAI_EMBEDDING_MODEL
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
        self.client: Optional[chromadb.ClientAPI] = None
        self.collection: Optional[chromadb.Collection] = None
        self._initialized = False
        
        # –ö–µ—à –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Ñ–∞–π–ª–∞—Ö
        self._file_hashes: Dict[str, str] = {}
        
    def _ensure_directory(self) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è ChromaDB –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        if not os.path.exists(self.db_path):
            os.makedirs(self.db_path, exist_ok=True)
            logger.info("Created ChromaDB directory", path=self.db_path)
    
    def _initialize_client(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç ChromaDB"""
        if self.client is not None:
            return
            
        try:
            self._ensure_directory()
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç ChromaDB
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                self.collection = self.client.get_collection(name=self.collection_name)
                logger.info("Connected to existing ChromaDB collection", 
                           collection=self.collection_name)
            except Exception:
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "Casino knowledge base vectors"}
                )
                logger.info("Created new ChromaDB collection", 
                           collection=self.collection_name)
            
            self._initialized = True
            
        except Exception as e:
            logger.error("Failed to initialize ChromaDB client", error=str(e))
            raise VectorServiceError(f"ChromaDB initialization failed: {e}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return hashlib.sha256(content.encode('utf-8')).hexdigest()
        except Exception as e:
            logger.warning("Failed to compute file hash", file=file_path, error=str(e))
            return ""
    
    def _should_rebuild_vectors(self, kb_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä—ã"""
        if not config.VECTOR_REBUILD_ON_CHANGES:
            return False
            
        if not os.path.exists(kb_path):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ txt —Ñ–∞–π–ª–∞—Ö
        current_hashes = {}
        for root, dirs, files in os.walk(kb_path):
            for file in files:
                if file.endswith('.txt'):
                    file_path = os.path.join(root, file)
                    current_hashes[file_path] = self._get_file_hash(file_path)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ö–µ—à–∞–º–∏
        if current_hashes != self._file_hashes:
            self._file_hashes = current_hashes
            logger.info("File changes detected, vectors will be rebuilt")
            return True
        
        return False
    
    def initialize_knowledge_base_sync(self, kb_path: str) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ txt —Ñ–∞–π–ª–æ–≤ (–°–ò–ù–•–†–û–ù–ù–û)
        1. –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª—ã –∏–∑ kb_path
        2. –†–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏
        3. –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ OpenAI API
        4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ChromaDB
        """
        try:
            logger.info("Initializing ChromaDB client")
            self._initialize_client()
            logger.info("ChromaDB client initialized successfully")
            
            if not os.path.exists(kb_path):
                logger.warning("Knowledge base path does not exist", path=kb_path)
                return
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä—ã
            if not self._should_rebuild_vectors(kb_path) and self.collection.count() > 0:
                logger.info("Vectors are up to date, skipping rebuild")
                return
            
            logger.info("Initializing knowledge base", kb_path=kb_path)
            
            # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            if self.collection.count() > 0:
                self.client.delete_collection(self.collection_name)
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "Casino knowledge base vectors"}
                )
                logger.info("Cleared existing vectors")
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ txt —Ñ–∞–π–ª—ã
            documents = []
            metadatas = []
            ids = []
            doc_counter = 0
            
            for root, dirs, files in os.walk(kb_path):
                for file in files:
                    if not file.endswith('.txt'):
                        continue
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª –ø—Ä–æ–º–æ–∞–∫—Ü–∏–π, –µ—Å–ª–∏ —Ä–∞–∑–º–µ—â–µ–Ω –≤ kb/
                    if file.lower() == os.path.basename(config.PROMOTIONS_FILE).lower():
                        logger.debug("Skipping promotions file from embeddings", file=file)
                        continue
                        
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
                        lines = [line.strip() for line in content.split('\n') if line.strip()]
                        
                        for line_num, line in enumerate(lines):
                            if len(line) < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                                continue
                                
                            doc_counter += 1
                            documents.append(line)
                            ids.append(f"doc_{doc_counter}")
                            metadatas.append({
                                "file": file,
                                "file_path": file_path,
                                "line_number": line_num + 1,
                                "created_at": datetime.now().isoformat()
                            })
                            
                    except Exception as e:
                        logger.warning("Failed to read file", file=file_path, error=str(e))
                        continue
            
            if not documents:
                logger.warning("No documents found in knowledge base")
                return
            
            logger.info("Processing documents", total_documents=len(documents))
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–∞–∫–µ—Ç–∞–º–∏
            try:
                logger.info("Creating embeddings via OpenAI API", document_count=len(documents))
                embeddings = embedding_service.create_embeddings_batch_sync(documents)
                logger.info("Embeddings created successfully", embedding_count=len(embeddings))
                
                if len(embeddings) != len(documents):
                    raise VectorServiceError("Mismatch between documents and embeddings count")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ChromaDB
                logger.info("Saving embeddings to ChromaDB", vector_count=len(embeddings))
                self.collection.add(
                    documents=documents,
                    embeddings=embeddings,
                    metadatas=metadatas,
                    ids=ids
                )
                logger.info("Embeddings saved to ChromaDB successfully")
                
                logger.info("Knowledge base initialized successfully",
                           total_documents=len(documents),
                           total_embeddings=len(embeddings))
                
            except EmbeddingServiceError as e:
                logger.error("Failed to create embeddings", error=str(e))
                raise VectorServiceError(f"Embedding creation failed: {e}")
            
        except Exception as e:
            logger.error("Failed to initialize knowledge base", error=str(e))
            raise VectorServiceError(f"Knowledge base initialization failed: {e}")
    
    async def search_similar(self, query: str, limit: int = None, threshold: float = None) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        1. –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        2. –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –≤ ChromaDB
        3. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        try:
            self._initialize_client()
            
            if not query.strip():
                return []
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            search_limit = limit or config.VECTOR_SEARCH_LIMIT
            similarity_threshold = threshold or config.VECTOR_SIMILARITY_THRESHOLD
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            if self.collection.count() == 0:
                logger.warning("No documents in vector database")
                return []
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await embedding_service.create_embedding(query)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ ChromaDB
            search_results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=search_limit,
                include=["documents", "metadatas", "distances"]
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            if search_results["documents"] and search_results["documents"][0]:
                documents = search_results["documents"][0]
                metadatas = search_results["metadatas"][0] if search_results["metadatas"] else []
                distances = search_results["distances"][0] if search_results["distances"] else []
                
                for i, document in enumerate(documents):
                    # ChromaDB –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ö–æ–∂–µ—Å—Ç—å (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
                    distance = distances[i] if i < len(distances) else 1.0
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ [0,1] —Å —É—á–µ—Ç–æ–º –º–µ—Ç—Ä–∏–∫–∏ cosine (–æ–±—ã—á–Ω–æ [0,2])
                    if distance <= 1.0:
                        # –£–∂–µ –≤ —Ä–∞–∑—É–º–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0,1]
                        similarity = 1.0 - distance
                    elif distance <= 2.0:
                        # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è "distance" –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ [0,2]; –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                        similarity = 1.0 - (distance / 2.0)
                    else:
                        # –ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî –¥–∞–µ–º –æ—á–µ–Ω—å –Ω–∏–∑–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
                        similarity = max(0.0, 1.0 / (1.0 + distance))
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–∂–µ—Å—Ç–∏
                    if similarity < similarity_threshold:
                        continue
                    
                    metadata = metadatas[i] if i < len(metadatas) else {}
                    
                    results.append({
                        "content": document,
                        "similarity": similarity,
                        "distance": distance,
                        "metadata": metadata
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
            results.sort(key=lambda x: x["similarity"], reverse=True)
            
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            logger.info("=== VECTOR SEARCH RESULTS ===",
                       query=query[:100],
                       results_found=len(results),
                       threshold=similarity_threshold,
                       limit=search_limit)
            
            if results:
                logger.info("üìÑ Found documents:")
                for i, result in enumerate(results[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                    content = result.get("content", "")[:100]
                    similarity = result.get("similarity", 0.0)
                    metadata = result.get("metadata", {})
                    file_name = metadata.get("file", "unknown")
                    
                    logger.info(f"  [{i+1}] Score: {similarity:.3f} | File: {file_name} | Distance: {result.get('distance', 0):.3f}")
                    logger.info(f"      Content: '{content}...'")
                
                if len(results) > 3:
                    logger.info(f"  ... and {len(results) - 3} more documents")
            else:
                logger.warning("‚ùå No documents found above similarity threshold",
                             threshold=similarity_threshold)
            
            logger.info("=== END VECTOR SEARCH ===")
            
            logger.debug("Vector search completed",
                        query=query[:50],
                        results_count=len(results),
                        threshold=similarity_threshold,
                        limit=search_limit)
            
            return results
            
        except EmbeddingServiceError as e:
            logger.error("Failed to create query embedding", query=query[:50], error=str(e))
            raise VectorServiceError(f"Query embedding failed: {e}")
        except Exception as e:
            logger.error("Vector search failed", query=query[:50], error=str(e))
            raise VectorServiceError(f"Vector search failed: {e}")
    
    async def add_document(self, content: str, metadata: Dict) -> str:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ChromaDB
        """
        try:
            self._initialize_client()
            
            if not content.strip():
                raise VectorServiceError("Document content cannot be empty")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_id = f"doc_{int(datetime.now().timestamp() * 1000000)}"
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            embedding = await embedding_service.create_embedding(content)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            full_metadata = {
                **metadata,
                "added_at": datetime.now().isoformat(),
                "content_length": len(content)
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ChromaDB
            self.collection.add(
                documents=[content],
                embeddings=[embedding],
                metadatas=[full_metadata],
                ids=[doc_id]
            )
            
            logger.debug("Document added to vector database",
                        doc_id=doc_id,
                        content_length=len(content))
            
            return doc_id
            
        except EmbeddingServiceError as e:
            logger.error("Failed to create embedding for document", error=str(e))
            raise VectorServiceError(f"Document embedding failed: {e}")
        except Exception as e:
            logger.error("Failed to add document", error=str(e))
            raise VectorServiceError(f"Failed to add document: {e}")
    
    async def health_check(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
        
        Returns:
            True –µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω, False –∏–Ω–∞—á–µ
        """
        try:
            self._initialize_client()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ChromaDB
            if not self.client or not self.collection:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            embedding_health = await embedding_service.health_check()
            
            return embedding_health
            
        except Exception as e:
            logger.warning("Vector service health check failed", error=str(e))
            return False


class VectorServiceError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
    pass


def get_vector_service() -> VectorServiceInterface:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
    return ChromaVectorService()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
vector_service = get_vector_service()
